<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>NLP | 基于朴素贝叶斯的中文文本分类</title>
    <link href="/blogs/2022/05/14/NLP-%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <url>/blogs/2022/05/14/NLP-%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="基于朴素贝叶斯的中文文本分类"><a href="#基于朴素贝叶斯的中文文本分类" class="headerlink" title="基于朴素贝叶斯的中文文本分类"></a>基于朴素贝叶斯的中文文本分类</h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>根据训练集建立不同类别的一元和二元语法，使用朴素贝叶斯分类模型进行测试集文本分类</p><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><blockquote><p>1.根据训练集建立不同类别的一元和二元语法，使用朴素贝叶斯分类模型进行测试集文本分类；</p><p>2.语言模型使用困惑度，分类使用准确率，召回率，F1进行性能评估；</p><p>3.代码：python为主</p></blockquote><h2 id="实验原理及实现思路"><a href="#实验原理及实现思路" class="headerlink" title="实验原理及实现思路"></a>实验原理及实现思路</h2><p>建立一元语法和二元语法，使用朴素贝叶斯模型进行测试集文本分类。</p><h3 id="step1-分词"><a href="#step1-分词" class="headerlink" title="step1.分词"></a>step1.分词</h3><p>分词采用的jieba进行分词，也自己实现了基于词表的逆向最大匹配法进行分词，效果一般但是处理速度太慢了，所以最终还是采用jieba分词实现。<br>这里简单说明一下两个的实现思路或实现代码。逆向最大匹配是从后到前搜索字符串，然后找到最长的匹配结果输出。</p><p>逆向最大匹配</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">segmentation_back</span>(<span class="hljs-params">self, sentence</span>):<br>    max_len = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> self.words_dic)<br>    sentence = sentence.strip()<br><br>    result = []<br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(sentence) &gt; <span class="hljs-number">0</span>:<br>        max_cut_len = <span class="hljs-built_in">min</span>(max_len, <span class="hljs-built_in">len</span>(sentence))<br>        sub_sentence = sentence[-max_cut_len:]<br>        <span class="hljs-keyword">while</span> max_cut_len &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">if</span> sub_sentence <span class="hljs-keyword">in</span> self.words_dic:<br>                result.append(sub_sentence)<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">elif</span> max_cut_len == <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">if</span> sub_sentence <span class="hljs-keyword">in</span> self.words_dic:<br>                    result.append(sub_sentence)<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">else</span>:<br>                max_cut_len -= <span class="hljs-number">1</span><br>                sub_sentence = sub_sentence[-max_cut_len:]<br>        sentence = sentence[<span class="hljs-number">0</span>:-max_cut_len]<br>    result.reverse()<br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><p>使用jieba分词</p><p>lcut返回分好词的list形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">raw = jieba.lcut(sentence)<br></code></pre></td></tr></table></figure><h3 id="step2-统计并计算概率"><a href="#step2-统计并计算概率" class="headerlink" title="step2.统计并计算概率"></a>step2.统计并计算概率</h3><h4 id="一元语法"><a href="#一元语法" class="headerlink" title="一元语法"></a>一元语法</h4><p>计算单词wi在类别c中的概率，加上laplace平滑之后的公式如下</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_1.png?raw=true" alt="pic/img_1.png"></p><p>对于整个句子<br>$$<br>p(x_1,x_2,…,x_n|c)&#x3D;p(x_1|c)<em>p(x_2|c)</em>…*p(x_n|c)<br>$$</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_2.png?raw=true" alt="pic/img_2.png"></p><p>一元语法中，因为p(xi|c)相互独立，所以做了去除停用词和二值化处理。为了加速统计，减少遍历次数，也做了打标的操作，table字典中储存形式为[单词，类别为1的概率，类别为0的概率]。</p><p>实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">getWkOfCj</span>(<span class="hljs-params">wk, cj</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    计算某单词wk为类别cj的可能性 p（wk|cj）</span><br><span class="hljs-string">    :param wk: </span><br><span class="hljs-string">    :param cj: </span><br><span class="hljs-string">    :return: </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># count(w,c)</span><br>    cnt_wc = <span class="hljs-number">0</span><br>    cnt_c = <span class="hljs-number">0</span><br>    cnt_all = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_data[<span class="hljs-string">&quot;label&quot;</span>])):<br>        cnt_all += <span class="hljs-built_in">sum</span>(train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i].values())<br>        <span class="hljs-keyword">if</span> train_data[<span class="hljs-string">&quot;label&quot;</span>][i] == cj:<br>            cnt_wc += train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i][wk]<br>            cnt_c += <span class="hljs-built_in">sum</span>(train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i].values())<br>    p = np.log((cnt_wc + <span class="hljs-number">1</span>) / (cnt_c + <span class="hljs-built_in">len</span>(words_dic)))<br>    <span class="hljs-keyword">return</span> p<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">getProbOfCi</span>(<span class="hljs-params">words, i</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    计算某句子word为类别i的可能性</span><br><span class="hljs-string">    :param words: </span><br><span class="hljs-string">    :param i: </span><br><span class="hljs-string">    :return: </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    label_count = Counter(train_data[<span class="hljs-string">&quot;label&quot;</span>])<br>    p = np.log(label_count[i] / <span class="hljs-built_in">sum</span>(label_count.values()))<br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words:<br>        <span class="hljs-keyword">if</span> w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> table.keys():<br>            <span class="hljs-comment"># print(table[&quot;word&quot;].values)</span><br>            pw1 = getWkOfCj(w, <span class="hljs-number">1</span>)<br>            pw0 = getWkOfCj(w, <span class="hljs-number">0</span>)<br>            table[w] = [pw1, pw0]<br>            p += pw1 <span class="hljs-keyword">if</span> i == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> pw0<br>        <span class="hljs-keyword">else</span>:<br>            line = table[w]<br>            p += line[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> i == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> line[<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">return</span> p<br></code></pre></td></tr></table></figure><h4 id="二元语法：假设下一个词的出现依赖于它前面的一个词"><a href="#二元语法：假设下一个词的出现依赖于它前面的一个词" class="headerlink" title="二元语法：假设下一个词的出现依赖于它前面的一个词"></a>二元语法：假设下一个词的出现依赖于它前面的一个词</h4><p>最大似然估计：<br><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_3.png?raw=true" alt="pic/img_3.png"></p><p>加上laplace平滑</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_4.png?raw=true" alt="pic/img_4.png"></p><p>而二元语法的朴素贝叶斯公式模型</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_9.png?raw=true" alt="pic/img_9.png"><br><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_8.png?raw=true" alt="pic/img_8.png"><br><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_6.png?raw=true" alt="pic/img_6.png"></p><p>统计二元语法的概率时，为了方便统计wi wi-1同时出现在类别c中的概率，处理方式为既加入单词词语也加入与上一个的连词，例如”今天 天气 真好“—&gt;START START今天 今天 今天天气 天气 天气真好 真好 真好END END。</p><p>同样，在这里为了加速减少遍历，也做了打表的操作，count_table中的储存形式为[词,词在类别1中的频次，词在类别2中的频次]。</p><p>实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">getWkOfCj</span>(<span class="hljs-params">wk, wk_1, cj</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    p(wi|wi-1)=count(wi-1 wi,c)+1/count(wi-1,c)+v</span><br><span class="hljs-string">    :param wk: </span><br><span class="hljs-string">    :param wk_1: </span><br><span class="hljs-string">    :param cj: </span><br><span class="hljs-string">    :return: </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    cnt_ww_1 = <span class="hljs-number">0</span><br>    cnt_w_1 = <span class="hljs-number">0</span><br>    cnt_ww_0 = <span class="hljs-number">0</span><br>    cnt_w_0 = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">if</span> wk + wk_1 <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> count_table.keys() <span class="hljs-keyword">or</span> wk_1 <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> count_table.keys():<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_data[<span class="hljs-string">&quot;label&quot;</span>])):  <span class="hljs-comment"># 遍历每一行</span><br>            <span class="hljs-keyword">if</span> train_data[<span class="hljs-string">&quot;label&quot;</span>][i] == <span class="hljs-number">1</span>:<br>                cnt_ww_1 += train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i][wk + wk_1]  <span class="hljs-comment"># wkwk-1在第i个句子中出现次数</span><br>                cnt_w_1 += train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i][wk_1]  <span class="hljs-comment"># wk-1在第i个句子中出现次数</span><br>            <span class="hljs-keyword">else</span>:<br>                cnt_ww_0 += train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i][wk + wk_1]  <span class="hljs-comment"># wkwk-1在第i个句子中出现次数</span><br>                cnt_w_0 += train_data[<span class="hljs-string">&quot;word_count&quot;</span>][i][wk_1]  <span class="hljs-comment"># wk-1在第i个句子中出现次数</span><br>        <span class="hljs-keyword">if</span> wk + wk_1 <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> count_table.keys():<br>            count_table[wk + wk_1] = [cnt_ww_1, cnt_ww_0]<br>        <span class="hljs-keyword">if</span> wk_1 <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> count_table.keys():<br>            count_table[wk_1] = [cnt_w_1, cnt_w_0]<br>    <span class="hljs-keyword">else</span>:<br>        cnt_ww_1, cnt_ww_0 = count_table[wk + wk_1]<br>        cnt_w_1, cnt_w_0 = count_table[wk_1]<br>    <span class="hljs-keyword">if</span> cj == <span class="hljs-number">1</span>:<br>        p = np.log((cnt_ww_1 + <span class="hljs-number">1</span>) / (cnt_w_1 + <span class="hljs-built_in">len</span>(words_dic)))  <span class="hljs-comment"># 平滑</span><br>    <span class="hljs-keyword">else</span>:<br>        p = np.log((cnt_ww_0 + <span class="hljs-number">1</span>) / (cnt_w_0 + <span class="hljs-built_in">len</span>(words_dic)))<br>    <span class="hljs-keyword">return</span> p<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">getProbOfCi</span>(<span class="hljs-params">words</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    一个句子为类别1和0的可能性</span><br><span class="hljs-string">    :param words: 某句子</span><br><span class="hljs-string">    :param i: 类别i</span><br><span class="hljs-string">    :return: p</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 类别概率</span><br>    label_count = Counter(train_data[<span class="hljs-string">&quot;label&quot;</span>])<br>    p0 = np.log(label_count[<span class="hljs-number">0</span>] / <span class="hljs-built_in">sum</span>(label_count.values()))<br>    p1 = np.log(label_count[<span class="hljs-number">1</span>] / <span class="hljs-built_in">sum</span>(label_count.values()))<br><br>    <span class="hljs-comment"># 添加首尾标志位</span><br>    words.insert(<span class="hljs-number">0</span>, <span class="hljs-string">&quot;START&quot;</span>)<br>    words.append(<span class="hljs-string">&quot;END&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words) - <span class="hljs-number">1</span>):<br>        pw0 = getWkOfCj(words[i], words[i + <span class="hljs-number">1</span>], <span class="hljs-number">0</span>)<br>        pw1 = getWkOfCj(words[i], words[i + <span class="hljs-number">1</span>], <span class="hljs-number">1</span>)<br>        p0 += pw0<br>        p1 += pw1<br>    <span class="hljs-keyword">return</span> p1, p0<br></code></pre></td></tr></table></figure><h3 id="step3-分类及评价指标"><a href="#step3-分类及评价指标" class="headerlink" title="step3.分类及评价指标"></a>step3.分类及评价指标</h3><p>类别预测已经在step2中的getProbOfCi()函数中实现。</p><p><strong>分类</strong></p><p>评价指标为p，r，f1</p><p>$$<br>P&#x3D;tp&#x2F;(tp+fp)<br>$$</p><p>$$<br>R&#x3D;tp&#x2F;(tp+fn)<br>$$</p><p>$$<br>F1&#x3D;2PR&#x2F;(R+P)<br>$$</p><p>实现代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculateARF</span>(<span class="hljs-params">result</span>):<br>    tp, fp, fn, tn = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(result)):<br>        <span class="hljs-keyword">if</span> test_data[<span class="hljs-string">&quot;label&quot;</span>][i] == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">if</span> result[i] == <span class="hljs-number">1</span>:<br>                tp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                fn += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> result[i] == <span class="hljs-number">1</span>:<br>                fp += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                tn += <span class="hljs-number">1</span><br>    accuracy = tp / (tp + fp)<br>    recall = tp / (tp + fn)<br>    f1 = <span class="hljs-number">2</span> * accuracy * recall / (accuracy + recall)<br>    <span class="hljs-built_in">print</span>(accuracy, recall, f1)<br></code></pre></td></tr></table></figure><p><strong>语言模型</strong></p><p>困惑度</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/img_11.png?raw=true" alt="pic/img_11.png"></p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculateComplex2Gram</span>(<span class="hljs-params">words</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    传入一句话，用计算二元语法困惑度</span><br><span class="hljs-string">    :param words:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    p = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words) - <span class="hljs-number">1</span>):<br>        p += np.log(<br>            (count_table[words[i] + words[i + <span class="hljs-number">1</span>]][<span class="hljs-number">0</span>] + count_table[words[i] + words[i + <span class="hljs-number">1</span>]][<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>) / (<br>                    count_table[words[i]][<span class="hljs-number">0</span>] + count_table[words[i]][<span class="hljs-number">1</span>] + <span class="hljs-built_in">len</span>(words_dic)))<br><br>    <span class="hljs-built_in">complex</span> = np.exp(-p / <span class="hljs-built_in">len</span>(words))<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">complex</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calculateComplex1Gram</span>(<span class="hljs-params">words</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    传入一句话,计算1元语法困惑度</span><br><span class="hljs-string">    :param words:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    p = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>        <span class="hljs-keyword">if</span> words[i] <span class="hljs-keyword">in</span> count_table.keys():<br>            p += np.log((count_table[words[i]][<span class="hljs-number">0</span>] + count_table[words[i]][<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>) / words_sum)<br>        <span class="hljs-keyword">else</span>:<br>            cnt = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_data[<span class="hljs-string">&quot;label&quot;</span>])):  <span class="hljs-comment"># 遍历每一行</span><br>                cnt += train_data[<span class="hljs-string">&quot;word_count&quot;</span>][j][words[i]]  <span class="hljs-comment"># wkwk-1在第i个句子中出现次数</span><br>            p += np.log(cnt / words_sum)<br>    <span class="hljs-built_in">complex</span> = np.exp(-p / <span class="hljs-built_in">len</span>(words))<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">complex</span><br><br></code></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>一元语法</p><table><thead><tr><th>P</th><th>R</th><th>F1</th><th>Complexity</th></tr></thead><tbody><tr><td>0.826</td><td>0.821</td><td>0.823</td><td>1246.928</td></tr></tbody></table><p>二元语法</p><table><thead><tr><th>P</th><th>R</th><th>F1</th><th>Complexity</th></tr></thead><tbody><tr><td>0.863</td><td>0.934</td><td>0.897</td><td>1095.262</td></tr></tbody></table><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/Wokeeeeee/nlp-courses/tree/master/task2%20text-classification">click here</a></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP | HMM中文分词及词性标注</title>
    <link href="/blogs/2022/05/14/NLP-HMM%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%8F%8A%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"/>
    <url>/blogs/2022/05/14/NLP-HMM%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%8F%8A%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="HMM实现中文分词和词性标注"><a href="#HMM实现中文分词和词性标注" class="headerlink" title="HMM实现中文分词和词性标注"></a>HMM实现中文分词和词性标注</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>分词和词性标注</p><p>1.使用分词工具(jieba等等)或者自己实现课程讲的任意分词和词性标注方法进行数据集的分词和词性标注实验；</p><p>2.对测试集分别进行分词和词性标注性能评估，评估指标至少包括准确率，召回率，F-测度；</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><p>时序的概率模型，描述由一个隐藏的马尔可夫链接随机生成不可观察的状态序列，再有状态序列生成一个观测序列的过程。</p><h4 id="μ-A-B-π"><a href="#μ-A-B-π" class="headerlink" title="μ(A,B,π)"></a><em>μ</em>(<em>A</em>,<em>B</em>,<em>π</em>)</h4><ul><li>初始状态概率向量：句子的第一个字属于BEMS四种状态的概率</li><li>状态转移概率矩阵：如果前一个字的位置是B，那么后一个字位置为BEMS的概率各是多少？</li><li>观测概率矩阵： 在状态b条件下，观察值为耀的概率</li></ul><h4 id="三类基本问题"><a href="#三类基本问题" class="headerlink" title="三类基本问题"></a>三类基本问题</h4><ol><li>给定模型<em>μ</em>&#x3D;(<em>A</em>,<em>B</em>,<em>π</em>)，计算某个观察序列<em>O&#x3D;o1o2 …… oT</em>的概率<em>P</em>(<em>O</em> | <em>μ</em>)&#x3D;?—&gt;向前算法</li><li>给定模型<em>μ</em>&#x3D;(<em>A</em>,<em>B</em>,π)和观察序列<em>O&#x3D;o1o2 …… oT</em> ，如何有效地确定一个状态序列<em>Q&#x3D;q1q2 …… qT</em> ，以便最好地解释观察序列？arg max⁡ P(Q | O)&#x3D;？—&gt; Viterbi算法</li><li>给定一个观察序列<em>O&#x3D;o1o2 …… oT</em> ，如何找到一个能够最好地解释这个观察序列的模型，即如何调节模型参数<em>μ</em>&#x3D;(<em>A</em>,<em>B</em>,π)，使得<em>P</em>(<em>O</em>| <em>μ</em>)最大化？—&gt;Baum-Welch算法</li></ol><h4 id="Viterbi"><a href="#Viterbi" class="headerlink" title="Viterbi"></a>Viterbi</h4><p>分词和词性标注都属于问题2，采用viterbi算法，用动态规划去求解最优路径。</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/hmm1.png?raw=true" alt="img"></p><p>如果从起点A经过P、H到达终点G是一条最短路径，那么，由A出发经过P到达H所走的这条子路径，对于从A出发到H的所有可能的路径来说，必定也是<strong>最短路径</strong></p><p>全局最优→局部最优</p><p>局部最优是全局最优的必要条件</p><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><h3 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h3><h4 id="BMES"><a href="#BMES" class="headerlink" title="BMES"></a>BMES</h4><p>汉语句子作为输入，BEMS序列串作为输出</p><ul><li>B-&gt;汉语的起始词</li><li>M-&gt;中间字</li><li>E-&gt;结束词</li><li>S-&gt;单字成词</li></ul><h4 id="基于HMM的中文分词"><a href="#基于HMM的中文分词" class="headerlink" title="基于HMM的中文分词"></a>基于HMM的中文分词</h4><ol><li><h5 id="导入已知分好的词序，训练HMM的三个矩阵。"><a href="#导入已知分好的词序，训练HMM的三个矩阵。" class="headerlink" title="导入已知分好的词序，训练HMM的三个矩阵。"></a>导入已知分好的词序，训练HMM的三个矩阵。</h5></li></ol><p>初始状态概率矩阵：统计所有训练样本中分别以状态S、B、M、E为初始状态的样本的数量，之后分别除以训练词语总数，就可以得到初始概率分布。</p><p>状态转移概率矩阵：统计所有样本中，从状态S转移到B的出现次数，再除以S出现的总次数，便得到由S转移到B的转移概率，其他同理。</p><p>观测概率矩阵：统计训练数据中，状态为j并观测为k的频数，除以训练数据中状态j出现的次数，其他同理。</p><p>先统计频数，然后得到概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">training</span>(<span class="hljs-params">self, observes, states</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(states)):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>            self.init_vec[states[<span class="hljs-number">0</span>]] += <span class="hljs-number">1</span><br>            self.state_count[states[<span class="hljs-number">0</span>]] += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            self.trans_mat[states[i - <span class="hljs-number">1</span>]][states[i]] += <span class="hljs-number">1</span><br>            self.state_count[states[i]] += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> observes[i] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.emit_mat[states[i]]:<br>            self.emit_mat[states[i]][observes[i]] = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            self.emit_mat[states[i]][observes[i]] += <span class="hljs-number">1</span><br>            <br>            <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_prob</span>(<span class="hljs-params">self</span>):<br>    init_vec_prob = &#123;&#125;<br>    trans_mat_prob = &#123;&#125;<br>    emit_mat_prob = &#123;&#125;<br>    <span class="hljs-comment"># 初始概率分布</span><br>    <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> self.init_vec:<br>        init_vec_prob[vec] = self.init_vec[vec] / <span class="hljs-built_in">sum</span>(self.init_vec.values())<br><br>    <span class="hljs-comment"># 状态转移概率分布</span><br>    <span class="hljs-keyword">for</span> key1 <span class="hljs-keyword">in</span> self.trans_mat:<br>        trans_mat_prob[key1] = &#123;&#125;<br>        <span class="hljs-keyword">for</span> key2 <span class="hljs-keyword">in</span> self.trans_mat[key1]:<br>            trans_mat_prob[key1][key2] = self.trans_mat[key1][key2] / (<br>                self.state_count[key1] <span class="hljs-keyword">if</span> self.state_count[key1] != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">max</span>(self.state_count.values()))<br>    <span class="hljs-comment"># 观察概率</span><br>    <span class="hljs-keyword">for</span> key1 <span class="hljs-keyword">in</span> self.emit_mat:<br>        emit_mat_prob[key1] = &#123;&#125;<br>        <span class="hljs-keyword">for</span> key2 <span class="hljs-keyword">in</span> self.emit_mat[key1]:<br>            emit_mat_prob[key1][key2] = self.emit_mat[key1][key2] / (<br>                self.state_count[key1] <span class="hljs-keyword">if</span> self.state_count[key1] != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">max</span>(self.state_count.values()))<br>    <span class="hljs-keyword">return</span> init_vec_prob, trans_mat_prob, emit_mat_prob<br></code></pre></td></tr></table></figure><ol><li><h5 id="将HMM模型的三个矩阵带入viterbi算法"><a href="#将HMM模型的三个矩阵带入viterbi算法" class="headerlink" title="将HMM模型的三个矩阵带入viterbi算法"></a>将HMM模型的三个矩阵带入viterbi算法</h5></li></ol><p>第二类问题：</p><p>给定模型<em>μ&#x3D;</em>(<em>A,B,π</em>)和观察序列<em>O&#x3D;o1o2 …… oT</em> ，如何确定最优的状态序列<em>Q&#x3D;q1q2 …… qT</em> </p><h6 id="最优路径求解"><a href="#最优路径求解" class="headerlink" title="最优路径求解"></a>最优路径求解</h6><p><strong>最优问题：</strong></p><p>如果从起点A经过P、H到达终点G是一条最短路径，那么，由A出发经过P到达H所走的这条子路径，对于从A出发到H的所有可能的路径来说，必定也是最短路径</p><p><strong>动态规划：</strong></p><ul><li>在每个节点中仅存储从起点到当前节点的最优路径</li><li>每增加一个节点，都把它跟各个前驱节点的最优路径连接起来，找出连接后的最优路径，仅把它存储起来</li></ul><h6 id="viterbi算法步骤"><a href="#viterbi算法步骤" class="headerlink" title="viterbi算法步骤"></a>viterbi算法步骤</h6><p><img src="https://github.com/Wokeeeeee/picb/blob/master/picGo/hmm2.png?raw=true" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">viterbi</span>(<span class="hljs-params">self, sequence, DEFAULT</span>):<br>    tab = [&#123;&#125;]<br>    path = &#123;&#125;<br><br>    init_vec, trans_mat, emit_mat = self.get_prob()<br>    <span class="hljs-comment"># init</span><br>    <span class="hljs-keyword">for</span> state <span class="hljs-keyword">in</span> self.states:<br>        tab[<span class="hljs-number">0</span>][state] = init_vec[state] * emit_mat[state].get(sequence[<span class="hljs-number">0</span>])<br>        path[state] = [state]<br><br>    <span class="hljs-comment"># dp</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(sequence)):<br>        tab.append(&#123;&#125;)<br>        new_path = &#123;&#125;<br>        <span class="hljs-keyword">for</span> state1 <span class="hljs-keyword">in</span> STATES:<br>            items = []<br>            <span class="hljs-keyword">for</span> state2 <span class="hljs-keyword">in</span> STATES:  <span class="hljs-comment"># state2为后一个</span><br>                <span class="hljs-keyword">if</span> tab[t - <span class="hljs-number">1</span>][state2] == <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">continue</span><br>                <span class="hljs-keyword">else</span>:<br>                    prob = tab[t - <span class="hljs-number">1</span>][state2] * trans_mat[state2].get(state1, DEFAULT) * emit_mat[state1].get(sequence[t], DEFAULT)<br>                    items.append((prob, state2))<br>            best = <span class="hljs-built_in">max</span>(items)<br>            tab[t][state1] = best[<span class="hljs-number">0</span>]<br>            new_path[state1] = path[best[<span class="hljs-number">1</span>]] + [state1]<br>        path = new_path<br>    prob, state = <span class="hljs-built_in">max</span>([(tab[<span class="hljs-built_in">len</span>(sequence) - <span class="hljs-number">1</span>][state], state) <span class="hljs-keyword">for</span> state <span class="hljs-keyword">in</span> STATES])<br>    <span class="hljs-keyword">return</span> prob, state, path<br></code></pre></td></tr></table></figure><h4 id="NLP中的评价指标"><a href="#NLP中的评价指标" class="headerlink" title="NLP中的评价指标"></a>NLP中的评价指标</h4><p>$$<br>Precision&#x3D;\frac{|A\cap B|}{|B|};<br>$$</p><p>$$<br>Recall&#x3D;\frac{|A\cap B|}{|A|}<br>$$</p><p>$$<br>F-score&#x3D;\frac{2 \times Precision \times Recall}{Precision+Recall}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">prf</span>(<span class="hljs-params">gold: <span class="hljs-built_in">str</span>, pred: <span class="hljs-built_in">str</span></span>):<br>    A_size, B_size, A_cap_B_size, OOV, IV, OOV_R, IV_R = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    A, B = <span class="hljs-built_in">set</span>(to_region(gold)), <span class="hljs-built_in">set</span>(to_region(pred))<br>    A_size += <span class="hljs-built_in">len</span>(A)<br>    B_size += <span class="hljs-built_in">len</span>(B)<br>    A_cap_B_size += <span class="hljs-built_in">len</span>(A &amp; B)<br>    p, r = A_cap_B_size / B_size * <span class="hljs-number">100</span>, A_cap_B_size / A_size * <span class="hljs-number">100</span><br>    <span class="hljs-keyword">return</span> p, r, <span class="hljs-number">2</span> * p * r / (p + r)<br></code></pre></td></tr></table></figure><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h5 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h5><p>训练得到的初始矩阵和状态转移矩阵概率.</p><table><thead><tr><th>trans_mat_prob</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>S</td><td>M</td><td>B</td><td>E</td></tr><tr><td>S</td><td>0.38441</td><td>0.0</td><td>0.60212</td><td>0.0</td></tr><tr><td>M</td><td>0.0</td><td>0.31506</td><td>0.0</td><td>0.68493</td></tr><tr><td>B</td><td>0.0</td><td>0.14315</td><td>0.0</td><td>0.85684</td></tr><tr><td>E</td><td>0.37346</td><td>0.0</td><td>0.60165</td><td>0.0</td></tr></tbody></table><table><thead><tr><th>init_vec_prob</th><th></th></tr></thead><tbody><tr><td>S</td><td>0.346124</td></tr><tr><td>M</td><td>0</td></tr><tr><td>B</td><td>0.653875</td></tr><tr><td>E</td><td>0</td></tr></tbody></table><h5 id="分词序列"><a href="#分词序列" class="headerlink" title="分词序列"></a>分词序列</h5><blockquote><p>民族 复兴 迫切 需要 培养造 就 德才 兼备 的 人才 。 希望 你们 继续 发扬 严谨 治学 、 甘为 人梯 的 精神 ， 坚持 特色 、 争创 一流 ， 培养 更 多 具有 为国 奉献 钢筋 铁骨 的 高 素质 人才 ， 促进 钢铁 产业 创新 发展 、 绿色 低碳 发展 ， 为 铸 就 科技 强国 、 制造 强国 的 钢铁 脊梁 作出 新 的 更 大 的 贡献 ！</p></blockquote><h5 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h5><p>jieba分词结果作为标准</p><table><thead><tr><th>P</th><th>85.93</th></tr></thead><tbody><tr><td>R</td><td>76.39</td></tr><tr><td>F1</td><td>80.88</td></tr></tbody></table><h3 id="基于HMM的中文词性标注"><a href="#基于HMM的中文词性标注" class="headerlink" title="基于HMM的中文词性标注"></a>基于HMM的中文词性标注</h3><p>数据集为课程提供的people-2014文件夹。在实际操作过程中，没有考虑符合词性的参数估计和词性标注。例如[人民&#x2F;n 生活&#x2F;vn 水平&#x2F;n]&#x2F;nz 只考虑了人民&#x2F;n 生活&#x2F;vn 水平&#x2F;n，没有考虑人民生活水平\nz。</p><p>也尝试了基于最大概率的标注方法，但效果不佳，且不能应对新词出现的问题，标注会产生\unknown。</p><h4 id="训练参数矩阵"><a href="#训练参数矩阵" class="headerlink" title="训练参数矩阵"></a>训练参数矩阵</h4><p>采用people-2014&#x2F;train&#x2F;0123文件夹下的所有txt训练，这里用了一个简单的遍历问价夹下的所有txt文件。</p><p>统计状态转移的频数，这里采用了”<strong>start</strong>“和”<strong>end</strong>“关键词来统计首尾位置上出现某词语的概率。由此，初始矩阵可以和概率转移矩阵合并。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">training</span>(<span class="hljs-params">infile, transDict, emitDict</span>):<br>    fdi = <span class="hljs-built_in">open</span>(infile, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fdi:<br>        infs = line.strip().split()<br>        infs = clearINFS(infs)<br>        wpList = [[<span class="hljs-string">&quot;__NONE__&quot;</span>, <span class="hljs-string">&quot;__start__&quot;</span>]] + [s.split(<span class="hljs-string">&quot;/&quot;</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> infs] + [[<span class="hljs-string">&quot;__NONE_&quot;</span>, <span class="hljs-string">&quot;__end__&quot;</span>]]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(wpList)):<br>            pre_pos = wpList[i - <span class="hljs-number">1</span>][<span class="hljs-number">1</span>]  <span class="hljs-comment"># 前面一个词性（隐藏状态 y_t-1）</span><br>            cur_pos = wpList[i][<span class="hljs-number">1</span>]  <span class="hljs-comment"># 当前词性状态 y_t</span><br>            word = wpList[i][<span class="hljs-number">0</span>]  <span class="hljs-comment"># 当前观测值(发射值) x_t</span><br>            <span class="hljs-keyword">if</span> word == <span class="hljs-string">&quot;&quot;</span> <span class="hljs-keyword">or</span> cur_pos == <span class="hljs-string">&quot;&quot;</span> <span class="hljs-keyword">or</span> pre_pos == <span class="hljs-string">&quot;&quot;</span>:<br>                <span class="hljs-keyword">continue</span><br>            add2transDict(pre_pos, cur_pos, transDict)  <span class="hljs-comment"># 统计转移频次</span><br>            add2emitDict(cur_pos, word, emitDict)  <span class="hljs-comment"># 统计发射频次</span><br>        add2transDict(<span class="hljs-string">&quot;__end__&quot;</span>, <span class="hljs-string">&quot;__end__&quot;</span>, transDict)<br>    fdi.close()<br></code></pre></td></tr></table></figure><p>在导出模型参数阶段将统计的频数转换为概率的对数，之后运算过程中求概率相乘即求对数相加。</p><h4 id="HMM标注"><a href="#HMM标注" class="headerlink" title="HMM标注"></a>HMM标注</h4><p>测试集采用的people-2014&#x2F;0123&#x2F;c1001-24200318.txt</p><p>和HMM分词思路相同，通过动态规划的方式寻找最优路径，从前一列的最优路径上继续延申下一词的最优路径。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs Prolog">def predict4one(words, gPosList, transDict, emitDict, results):<br>    if words == []:<br>        return<br>    prePosDictList = []<br>    for i in range(len(words)):  # 遍历单词<br>        prePosDict = &#123;&#125;<br>        for pos in gPosList:  # 遍历词性<br>            if i == <span class="hljs-number">0</span>:  # 初始矩阵<br>                trans_prob = transDict[<span class="hljs-string">&quot;__start__&quot;</span>][pos]<br>                emit_prob = getEmitProb(emitDict, pos, words[i])<br>                total_prob = trans_prob + emit_prob  <br>                prePosDict[pos] = [total_prob, <span class="hljs-string">&quot;__start__&quot;</span>]<br>            else:<br>                emit_prob = getEmitProb(emitDict, pos, words[i])<br>                max_total_prob = <span class="hljs-number">-10000000.0</span><br>                max_pre_pos = <span class="hljs-string">&quot;&quot;</span><br>                for pre_pos in prePosDictList[i - <span class="hljs-number">1</span>]:  <br># 动态规划：全局最大-&gt;局部最大-&gt;在前一次里面找最大的<br>                    pre_prob = prePosDictList[i - <span class="hljs-number">1</span>][pre_pos][<span class="hljs-number">0</span>]<br>                    trans_prob = transDict[pre_pos][pos]<br>                    total_prob = pre_prob + trans_prob + emit_prob<br>                    if max_pre_pos == <span class="hljs-string">&quot;&quot;</span> or total_prob &gt; max_total_prob:<br>                        max_total_prob = total_prob<br>                        max_pre_pos = pre_pos<br>                prePosDict[pos] = [max_total_prob, max_pre_pos]<br>        prePosDictList.append(prePosDict)<br>    max_total_prob = <span class="hljs-number">-10000000.0</span><br>    max_pre_pos = <span class="hljs-string">&quot;&quot;</span><br>    for pre_pos in prePosDictList[len(prePosDictList) - <span class="hljs-number">1</span>]:  # 最后一列<br>        pre_prob = prePosDictList[len(prePosDictList) - <span class="hljs-number">1</span>][pre_pos][<span class="hljs-number">0</span>]<br>        trans_prob = transDict[pre_pos][<span class="hljs-string">&quot;__end__&quot;</span>]<br>        total_prob = pre_prob + trans_prob  <br>        if max_pre_pos == <span class="hljs-string">&quot;&quot;</span> or total_prob &gt; max_total_prob:<br>            max_total_prob = total_prob<br>            max_pre_pos = pre_pos<br>    posList = [max_pre_pos]  # 最优路径<br>    indx = len(prePosDictList) - <span class="hljs-number">1</span><br>    max_pre_pos = prePosDictList[indx][max_pre_pos][<span class="hljs-number">1</span>]<br>    indx -= <span class="hljs-number">1</span><br>    while indx &gt;= <span class="hljs-number">0</span>:<br>        posList.append(max_pre_pos)<br>        max_pre_pos = prePosDictList[indx][max_pre_pos][<span class="hljs-number">1</span>]<br>        indx -= <span class="hljs-number">1</span><br>    if len(posList) == len(words):<br>        posList.reverse()<br>        for i in range(len(posList)):<br>            results.append(words[i] + <span class="hljs-string">&quot;/&quot;</span> + posList[i])<br></code></pre></td></tr></table></figure><h4 id="结果和分析"><a href="#结果和分析" class="headerlink" title="结果和分析"></a>结果和分析</h4><p>截取的一段分词结果：</p><blockquote><p>本报&#x2F;rz 北京&#x2F;ns 1月22日&#x2F;t 电&#x2F;n （&#x2F;w 记者&#x2F;nnt 朱剑红&#x2F;nr ）&#x2F;w 我国&#x2F;n “&#x2F;w 十二五&#x2F;m ”&#x2F;w 规划&#x2F;n 提出&#x2F;v 的&#x2F;ude1 24&#x2F;m 个&#x2F;q 主要&#x2F;b 指标&#x2F;n ，&#x2F;w 绝大部分&#x2F;m 的&#x2F;ude1 实施&#x2F;vn 进度&#x2F;n 好&#x2F;a 于&#x2F;p 预期&#x2F;vn ，&#x2F;w 氮氧化物&#x2F;n 排放&#x2F;vn 总量&#x2F;n 减少&#x2F;v 、&#x2F;w 化石&#x2F;n 能源&#x2F;n 占&#x2F;v 一次&#x2F;mq 能源&#x2F;n 消费&#x2F;vn 比重&#x2F;n 、&#x2F;w 单位&#x2F;n GDP&#x2F;x 能源&#x2F;n 消耗&#x2F;v 降低&#x2F;vn 、&#x2F;w 单位&#x2F;n GDP&#x2F;x 二氧化碳&#x2F;n 排放&#x2F;vn 降低&#x2F;vn 等&#x2F;udeng 四&#x2F;m 个&#x2F;q 指标&#x2F;n 完成&#x2F;v 的&#x2F;ude1 进度&#x2F;n 滞后&#x2F;v 于&#x2F;p 预期&#x2F;vn 。&#x2F;w 这&#x2F;rzv 是&#x2F;vshi 国家&#x2F;n 发改委&#x2F;nis 有关&#x2F;vn 负责人&#x2F;nnt 今天&#x2F;t 在&#x2F;p “&#x2F;w 宏观&#x2F;n 经济&#x2F;n 形势&#x2F;n 和&#x2F;cc 政策&#x2F;n ”&#x2F;w 新闻&#x2F;n 发布会&#x2F;n 上&#x2F;f 透露&#x2F;v 的&#x2F;ude1 。&#x2F;w 国家&#x2F;n 发改委&#x2F;nis 已&#x2F;d 组织&#x2F;n 有关&#x2F;vn 部门&#x2F;n 和&#x2F;cc 有关方面&#x2F;nz 对&#x2F;p “&#x2F;w 十二五&#x2F;m ”&#x2F;w 规划&#x2F;n 的&#x2F;ude1 实施&#x2F;vn 情况&#x2F;n 进行&#x2F;vn 了&#x2F;ule 全面&#x2F;ad 的&#x2F;ude1 分析&#x2F;vn 和&#x2F;cc 评估&#x2F;vn ，&#x2F;w 评估&#x2F;vn 报告&#x2F;n 在&#x2F;p 修改&#x2F;v 完成&#x2F;vn 后&#x2F;f 将&#x2F;d 对外&#x2F;vn 公开&#x2F;ad 。&#x2F;w</p></blockquote><blockquote><p>在&#x2F;p 发展&#x2F;vn 目标&#x2F;n 方面&#x2F;n ，&#x2F;w “&#x2F;w 十二五&#x2F;m ”&#x2F;w 规划&#x2F;n 提出&#x2F;v 来&#x2F;vf 的&#x2F;ude1 GDP&#x2F;x 增长&#x2F;v 预期&#x2F;vn 性&#x2F;ng 指标&#x2F;n 是&#x2F;vshi 年均&#x2F;v 增长&#x2F;v 7%&#x2F;m ，&#x2F;w 在&#x2F;p 过去&#x2F;vf 三年&#x2F;t 里&#x2F;f ，&#x2F;w 分别&#x2F;d 实现&#x2F;v 了&#x2F;ule 9.2%&#x2F;m 、&#x2F;w 7.7%&#x2F;m 和&#x2F;cc 7.7%&#x2F;m ，&#x2F;w 完成&#x2F;v 目标&#x2F;n 没有&#x2F;v 任何&#x2F;rz 问题&#x2F;n 。&#x2F;w 其他&#x2F;rzv 指标&#x2F;n 有&#x2F;vyou 两个&#x2F;mq 已经&#x2F;d 提前&#x2F;vd 完成&#x2F;v ，&#x2F;w 一&#x2F;d 是&#x2F;vshi 每&#x2F;rz 万&#x2F;d 人&#x2F;n 发明&#x2F;v 专利&#x2F;n 拥有量&#x2F;n ，&#x2F;w 二&#x2F;m 是&#x2F;vshi 森林&#x2F;n 蓄积量&#x2F;nz 。&#x2F;w</p></blockquote><p>准确率为0.97440585</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux系统下串口号绑定</title>
    <link href="/blogs/2022/05/07/linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E4%B8%B2%E5%8F%A3%E5%8F%B7%E7%BB%91%E5%AE%9A/"/>
    <url>/blogs/2022/05/07/linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E4%B8%B2%E5%8F%A3%E5%8F%B7%E7%BB%91%E5%AE%9A/</url>
    
    <content type="html"><![CDATA[<h2 id="串口号绑定"><a href="#串口号绑定" class="headerlink" title="串口号绑定"></a>串口号绑定</h2><ol><li>通过串口类型绑定:此情况适用于每种串口类别(如<code>ttyUSB</code> ,<code>ttyACM</code>等)只有一个</li><li>通过硬件串口绑定:如果一种串口类型有多个设备接入,则可以通过设备的硬件端口来绑定</li></ol><p>无论是那种绑定方式,都需要完成三步:查看设备信息, 写rules文件和激活规则</p><p>两种绑定方式的区别不大,因为通过硬件串口绑定过程较多且包含了串口类型绑定的步骤,所以这里只介绍通过硬件串口的绑定方式.</p><h4 id="查看设备信息"><a href="#查看设备信息" class="headerlink" title="查看设备信息"></a>查看设备信息</h4><p>首先,在绑定串口之前,你需要知道你所绑定的设备的串口类别(如<code>ttyUSB</code> ,<code>video</code>,<code>ttyACM</code>等). 这里以绑定usb相机为例</p><p>在终端输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">ls</span> -l /dev | grep video<br></code></pre></td></tr></table></figure><p>终端会显示当前所有的video流,通过拔插usb相机来判断哪一个当前usb相机设备对应的video流</p><p>然后终端输入(以<code>video6</code>为例)</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">udevadm info -a <span class="hljs-regexp">/dev/</span>video6<br></code></pre></td></tr></table></figure><p><img src="https://github.com/Wokeeeeee/picb/blob/master/%E4%B8%B2%E5%8F%A3%E7%BB%91%E5%AE%9A/udevmadm.jpg?raw=true"></p><p>特别注意<code>KERNELS</code>,<code>ATTRS&#123;idVendor&#125;</code>,<code>ATTRS&#123;idProduct&#125;</code>的对应值</p><h4 id="写rules规则"><a href="#写rules规则" class="headerlink" title="写rules规则"></a>写rules规则</h4><p>在<code>/etc/udev/rules.d</code>里建立rules文件,</p><p>然后输入以下内容</p><p><img src="https://github.com/Wokeeeeee/picb/blob/master/%E4%B8%B2%E5%8F%A3%E7%BB%91%E5%AE%9A/rules.png?raw=true"></p><p>KERNEL这里写你设备的串口类型,<code>ATTRS&#123;idVendor&#125;</code>,<code>ATTRS&#123;idProduct&#125;</code>,KERNELS 从上面读取,MODE写你要赋予的权限,这里0777,其中<code>SYMLINK+=&quot;usb_cam&quot;</code>为建立的虚拟端口连接，打开相机的时候，将数字序号替换为<code>/dev/usb_cam</code>即可打开绑定的对应相机。</p><h4 id="激活规则"><a href="#激活规则" class="headerlink" title="激活规则"></a>激活规则</h4><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">sudo udevadm trigger</span><br></code></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo <span class="hljs-regexp">/etc/i</span>nit.d/udev restart<br></code></pre></td></tr></table></figure><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p><img src="https://github.com/Wokeeeeee/picb/blob/master/%E4%B8%B2%E5%8F%A3%E7%BB%91%E5%AE%9A/effect.png?raw=true"></p>]]></content>
    
    
    
    <tags>
      
      <tag>实物</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hello world</title>
    <link href="/blogs/2022/04/26/hello-world/"/>
    <url>/blogs/2022/04/26/hello-world/</url>
    
    <content type="html"><![CDATA[<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1><p>I’m trying to share some tech-info here, mainly for myself.</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
